{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Select what features you'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### features_list is a list of strings, each of which is a feature name.\n",
    "features_list = ['poi','salary', 'deferral_payments', 'total_payments', 'loan_advances',\\\n",
    "    'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value',\\\n",
    "    'expenses', 'exercised_stock_options', 'other', 'long_term_incentive',\\\n",
    "    'restricted_stock', 'director_fees', 'to_messages', 'from_poi_to_this_person',\\\n",
    "    'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "\n",
    "### Load the dictionary containing the dataset, create DataFrame from dictionary\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 362096,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 362096,\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.pop('TOTAL', 0)\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK', 0) # from reading the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Create new feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New feature ratio_to_poi is added to data_dict.\n",
      "New feature ratio_from_poi is added to data_dict.\n",
      "New feature ratio_bonus_to_salary is added to data_dict.\n"
     ]
    }
   ],
   "source": [
    "def compute_ratio(numerator, denominator, ratio_list):\n",
    "    for name in data_dict:\n",
    "        if data_dict[name][denominator] == 'NaN' or data_dict[name][denominator] == 0 \\\n",
    "        or data_dict[name][numerator] == 'NaN':\n",
    "            data_dict[name][ratio_list] = 0\n",
    "        if data_dict[name][denominator] != 'NaN' and data_dict[name][denominator] != 0 \\\n",
    "        and data_dict[name][numerator] != 'NaN':\n",
    "            ratio = data_dict[name][numerator] / float(data_dict[name][denominator])\n",
    "            data_dict[name][ratio_list] = ratio\n",
    "        else:\n",
    "            data_dict[name][ratio_list] = 'NaN'\n",
    "    print \"New feature\", ratio_list, \"is added to data_dict.\"\n",
    "\n",
    "compute_ratio('from_this_person_to_poi', 'to_messages', 'ratio_to_poi')\n",
    "compute_ratio('from_poi_to_this_person', 'from_messages', 'ratio_from_poi') \n",
    "compute_ratio('bonus', 'salary', 'ratio_bonus_to_salary')\n",
    "features_list.extend(('ratio_to_poi', 'ratio_from_poi', 'ratio_bonus_to_salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, remove_NaN=True, remove_all_zeroes=True, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Try a varity of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'sklearn.naive_bayes.GaussianNB'>, 'Accuracy:', '0.825')\n",
      "(<class 'sklearn.svm.classes.SVC'>, 'Accuracy:', '1.000')\n",
      "(<class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'Accuracy:', '1.000')\n",
      "(<class 'sklearn.neighbors.classification.KNeighborsClassifier'>, 'Accuracy:', '0.902')\n",
      "(<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'Accuracy:', '1.000')\n",
      "(<class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'Accuracy:', '0.965')\n"
     ]
    }
   ],
   "source": [
    "def clf_accuracy(classifier):\n",
    "    clf = classifier()\n",
    "    clf.fit(features, labels)\n",
    "    pred = clf.predict(features)\n",
    "    return classifier, \"Accuracy:\", \"%.3f\" % accuracy_score(pred, labels)\n",
    "\n",
    "### All classifiers are overfit\n",
    "print clf_accuracy(GaussianNB)\n",
    "print clf_accuracy(SVC)\n",
    "print clf_accuracy(DecisionTreeClassifier)\n",
    "print clf_accuracy(neighbors.KNeighborsClassifier)\n",
    "print clf_accuracy(AdaBoostClassifier)\n",
    "print clf_accuracy(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Fill all missing values with zero, so they can be discounted by MinMaxScaler\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index', dtype=np.float)\n",
    "df = df.replace('NaN', 0)\n",
    "\n",
    "### Remove string data in column email_address\n",
    "del df['email_address']\n",
    "\n",
    "### Transform all the features into floating points and seperate the label\n",
    "features_df = df.drop(['poi'], axis=1)\n",
    "labels_df = df['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "features_rescaled = scaler.fit_transform(features_df)\n",
    "\n",
    "### Update features array, turn labels into array\n",
    "features = features_rescaled\n",
    "labels = labels_df.as_matrix().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Use SelectKBest to find the cut-off point in the number of features selected\n",
    "### Chi-square scores work better with normalized data\n",
    "### Hard-coded the features according to the results in the write-up because SelectKBest produces different outcomes\n",
    "\n",
    "#skb = SelectKBest(chi2, k='all')\n",
    "#fit = skb.fit(features, labels)\n",
    "\n",
    "#temp = features_list[1:]\n",
    "#d = {'features': temp, 'scores': fit.scores_}\n",
    "#chi2_df = pd.DataFrame(d).sort_values(by='scores', ascending=False)\n",
    "#chi2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Visualize the score of each data, select cut-off point at the sharpest decline\n",
    "#chi2_df.plot.bar(x='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_selected = ['poi', 'bonus', 'long_term_incentive', 'exercised_stock_options', 'restricted_stock_deferred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset\n",
    "data = featureFormat(my_dataset, features_selected, sort_keys = True, remove_all_zeroes = True, remove_NaN = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "### Use StratifiedShuffleSplit to maximize the \n",
    "sss = StratifiedShuffleSplit(labels, n_iter=100, test_size=0.25, train_size=0.6, random_state=46)\n",
    "for train_idx, test_idx in sss: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append(features[ii] )\n",
    "            labels_train.append(labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append(features[jj] )\n",
    "            labels_test.append(labels[jj] )\n",
    "    \n",
    "print len(labels_train)\n",
    "print len(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator:  DecisionTreeClassifier(class_weight={False: 1, True: 8}, criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random')\n",
      "Best Params:  {'min_samples_split': 2, 'splitter': 'random', 'class_weight': {False: 1, True: 8}}\n",
      "DecisionTreeClassifier(class_weight={False: 1, True: 8}, criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random')\n",
      "\tAccuracy: 0.79050\tPrecision: 0.32455\tRecall: 0.31520\tF1: 0.31981\tF2: 0.31703\n",
      "\tTotal predictions: 32000\tTrue positives: 1576\tFalse positives: 3280\tFalse negatives: 3424\tTrue negatives: 23720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Try DecisionTree, no need to perform feature scaling\n",
    "clf = DecisionTreeClassifier()\n",
    "params = {'min_samples_split': range(2, 11),\n",
    "         'class_weight': ['balanced', {True: 12, False: 1}, {True: 10, False: 1}, {True: 8, False: 1}],\n",
    "         'splitter': ['random', 'best']\n",
    "         }\n",
    "clf_gs = GridSearchCV(clf, param_grid=params, scoring='f1')\n",
    "clf_gs.fit(features_train, labels_train)\n",
    "clf_estimator = clf_gs.best_estimator_\n",
    "clf_params = clf_gs.best_params_\n",
    "print \"Best Estimator: \", clf_estimator\n",
    "print \"Best Params: \", clf_params\n",
    "test_classifier(clf_estimator, my_dataset, features_selected, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyu/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator:  RandomForestClassifier(bootstrap=True, class_weight={False: 1, True: 12},\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=5,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Best Params:  {'min_samples_split': 5, 'n_estimators': 1, 'criterion': 'gini', 'class_weight': {False: 1, True: 12}}\n",
      "RandomForestClassifier(bootstrap=True, class_weight={False: 1, True: 12},\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=5,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\tAccuracy: 0.76197\tPrecision: 0.29754\tRecall: 0.38460\tF1: 0.33551\tF2: 0.36334\n",
      "\tTotal predictions: 32000\tTrue positives: 1923\tFalse positives: 4540\tFalse negatives: 3077\tTrue negatives: 22460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "params = {'n_estimators': range(1, 10),\n",
    "          'criterion': ['gini', 'entropy'],\n",
    "          'min_samples_split': range(1, 6, 2),\n",
    "          'class_weight': ['balanced', {True: 12, False: 1}, {True: 10, False: 1}, {True: 8, False: 1}]\n",
    "          }\n",
    "forest_gs = GridSearchCV(forest, param_grid=params, scoring='f1')\n",
    "forest_gs.fit(features, labels)\n",
    "forest_estimator = forest_gs.best_estimator_\n",
    "forest_params = forest_gs.best_params_\n",
    "print \"Best Estimator: \", forest_estimator\n",
    "print \"Best Params: \", forest_params\n",
    "test_classifier(forest_estimator, my_dataset, features_selected, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator:  Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform'))])\n",
      "Best Params:  {'knn__algorithm': 'auto', 'knn__weights': 'uniform', 'knn__n_neighbors': 3}\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform'))])\n",
      "\tAccuracy: 0.82441\tPrecision: 0.33855\tRecall: 0.12980\tF1: 0.18765\tF2: 0.14806\n",
      "\tTotal predictions: 32000\tTrue positives:  649\tFalse positives: 1268\tFalse negatives: 4351\tTrue negatives: 25732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "knn = KNeighborsClassifier()\n",
    "pipeline = Pipeline([('scaler', scaler), ('knn', knn)])\n",
    "params = {'knn__n_neighbors': range(1, 10),\n",
    "          'knn__weights': ['uniform', 'distance'],\n",
    "          'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "          }\n",
    "knn_gs = GridSearchCV(pipeline, param_grid=params, scoring='recall')\n",
    "knn_gs.fit(features, labels)\n",
    "knn_estimator = knn_gs.best_estimator_\n",
    "knn_params = knn_gs.best_params_\n",
    "print \"Best Estimator: \", knn_estimator\n",
    "print \"Best Params: \", knn_params\n",
    "test_classifier(knn_estimator, my_dataset, features_selected, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator:  Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('svc', SVC(C=22, cache_size=200, class_weight={False: 1, True: 10}, coef0=0.0,\n",
      "  decision_function_shape=None, degree=1, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Best Params:  {'svc__class_weight': {False: 1, True: 10}, 'svc__degree': 1, 'svc__kernel': 'rbf', 'svc__C': 22}\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('svc', SVC(C=22, cache_size=200, class_weight={False: 1, True: 10}, coef0=0.0,\n",
      "  decision_function_shape=None, degree=1, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.58109\tPrecision: 0.22649\tRecall: 0.69600\tF1: 0.34176\tF2: 0.49201\n",
      "\tTotal predictions: 32000\tTrue positives: 3480\tFalse positives: 11885\tFalse negatives: 1520\tTrue negatives: 15115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "scaler = MinMaxScaler()\n",
    "pipe = Pipeline([('scaler', scaler), ('svc', svc)])\n",
    "params = {'svc__kernel':['rbf', 'linear'], \n",
    "          'svc__degree': range(1, 3),\n",
    "          'svc__C': range(8, 25, 2),\n",
    "          'svc__class_weight': [{True: 14, False: 1}, {True: 12, False: 1}, {True: 10, False: 1}]\n",
    "          }\n",
    "svc_gs = GridSearchCV(pipe, param_grid=params, scoring='precision', cv=sss)\n",
    "svc_gs.fit(features, labels)\n",
    "svc_estimator = svc_gs.best_estimator_\n",
    "svc_params = svc_gs.best_params_\n",
    "print \"Best Estimator: \", svc_estimator\n",
    "print \"Best Params: \", svc_params\n",
    "test_classifier(svc_estimator, my_dataset, features_selected, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test one newly implemented feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator:  DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random')\n",
      "Best Params:  {'min_samples_split': 3, 'splitter': 'random', 'class_weight': 'balanced'}\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random')\n",
      "\tAccuracy: 0.77200\tPrecision: 0.29441\tRecall: 0.32880\tF1: 0.31066\tF2: 0.32129\n",
      "\tTotal predictions: 32000\tTrue positives: 1644\tFalse positives: 3940\tFalse negatives: 3356\tTrue negatives: 23060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Ratio-bonus_to_salary\n",
    "features_selected.append('ratio_bonus_to_salary')\n",
    "\n",
    "data = featureFormat(my_dataset, features_selected, sort_keys = True, remove_all_zeroes = True, remove_NaN = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "sss = StratifiedShuffleSplit(labels, n_iter=100, test_size=0.25, train_size=0.6, random_state=46)\n",
    "for train_idx, test_idx in sss: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append(features[ii] )\n",
    "            labels_train.append(labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append(features[jj] )\n",
    "            labels_test.append(labels[jj] )\n",
    "test = DecisionTreeClassifier()\n",
    "params = {'min_samples_split': range(2, 11),\n",
    "         'class_weight': ['balanced', {True: 12, False: 1}, {True: 10, False: 1}, {True: 8, False: 1}],\n",
    "         'splitter': ['random', 'best']\n",
    "         }\n",
    "test_gs = GridSearchCV(clf, param_grid=params, scoring='f1')\n",
    "test_gs.fit(features_train, labels_train)\n",
    "test_estimator = test_gs.best_estimator_\n",
    "test_params = test_gs.best_params_\n",
    "print \"Best Estimator: \", test_estimator\n",
    "print \"Best Params: \", test_params\n",
    "test_classifier(test_estimator, my_dataset, features_selected, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### With the new feature ratio_salary_to_bonus, the DecisionTreeClassifier no longer satisfies the 0.3 requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: Dump your classifier, dataset, and features_list so anyone can check your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump_classifier_and_data(clf_estimator, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
